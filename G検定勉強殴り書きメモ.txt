

AI の活用が国の経済成長を牽引する柱の 1 つになるという共通認識から,先進国各国では国の経済成長戦略の一部に AI の研究開発戦略が盛り込まれるようになっている.
各国とその国の経済成長戦略の組み合わせ
日本 - 新産業構造ビジョン
英国 - RAS 2020 戦略
ドイツ - デジタル戦略2025
中国 - インターネットプラスAI3年行動実施法案



既存の学習済みニューラルネットワークモデルを活用する手法に 転移学習 と蒸留がある.
転移学習では,学習済みモデルに対して新たに別の課題を学習させることで,少量のデータセットかつ少ない計算量で高い性能のモデルを得ることができる.
また,蒸留は,学習済みの大規模モデルの入力と出力を小規模なモデルの教師データとして利用することで,少ない計算資源で従来のモデルと同程度の性能を実現することが可能となる.



正則化とは,機械学習の学習において汎化誤差をできるだけ小さくするための手法の総称である.
ディープニューラルネットワーク（DNN）の学習で一般に用いられる正則化の手法に　荷重減衰　があり,
(重み減衰・アーリーストッピング・複数クロスバリデーション(k-CV)・ドロップアウト・バッチ正則化)がDNNの過学習防止


誤差関数に重みの L2 ノルムを加えることで重みの発散を抑えることができる.
また,L2　リッジ回帰　 ノルムの代わりに L1 ノルムを用いる L1正則化は,　スパース正則化　の一種であり,
重要でないパラメータを 0 に近づけることができる.
 L1 正則化を回帰に利用した場合　,Lasso回帰　と呼ばれる.

L2をより一般的にしたものがチコノフ正則化


MNIST はアメリカの国立標準技術研究所 手書き

CIFAR-10　約8000万枚　ILSVRC2012で優勝したAlex Krizhevskyさん

ImageNet　約 1400 万枚の自然画像　動画データセット


アンプーリング　入力のアップサンプリング　プーリングの逆操作（study AIによると逆プーリングは間違いらしい）
アップサンプリング　16bitの信号を24bitや32bitに変換するもの
逆畳み込み層　畳込み層の逆操作　CNNの特徴量マップから画像生成をする場合の操作
これらの操作は画像セグメンテーションで使われる構造らしい。


平均二乗誤差関数　二乗和誤差関数
交差エントロピー誤差関数　cousera で出てきたlogの関数　分類問題の損失関数
誤差関数を定義して最小化するのが勾配降下法
(全てのデータの？)二乗和誤差を最小化するのが最小二乗法

KL ダイバージェンス 確立分布を直接学習するための損失関数

損失関数にパラメータの二乗ノルムを加えるとL2正則化



人間の脳における学習の枠組みに基づいた三つの学習が機械学習には存在する.一つ目は小脳の働きを模倣した（ア）である.これは学習者に対して,教師が間違いを指摘し,学習者が正しい解を得ることである.二つ 目は大脳皮質の働きを模倣した（イ）である.代表的な手法として主成分分析（PCA）などの次元圧縮手法がある.三つ目は大脳基底核の働きを模倣した（ウ）である.学習者は正解値でなく,行動した結果に基づいた報酬が与えられる.この報酬をなるべく大きくするように学習者が行動していく.
教師あり　教師なし　強化学習

（AI白書：p.364, p426）
（AI白書：p.364, p.426）
（AI白書：p.364, p.426）

1990 年代の音声認識は   隠れマルコフモデルHMM   による,音自体を判別するための音響モデルと,　Nグラム法　　による語と語のつながりを判別する言語モデルの両方でできている.
RNNの登場で前処理がいらなくなったことからRNNに置き換わっている


入力サイズ4×4、フィルタサイズ2×2、パディング0、ストライド2の場合、
( 入力の高さ　+ 2倍パディング - フィルタ高さ　) / スライド  + 1




グレースケール化
モノクロにする


画像処理の分野においては,減算正規化と除算正規化の処理を行う（ウ）などが前処理として利用され
局所コントラスト正規化


DQN
強化学習において,行動価値関数の関数近似に畳み込みニューラルネットワークを用いた手法


大規模なディープニューラルネットワーク（DNN）の学習では学習するべきパラメータ数が膨大となるため,処理の高速化が必要となる.2012 年に提案された分散並列技術である（ア）や画像処理に特化したプロセッサの（イ）は大規模なニューラルネットワークの学習を実現するために利用されてきた.また,大規模なニューラルネットワークの学習が困難となる原因の一つとして,ある層の入力がそれより下層の学習が進むにつれて変化する（ウ）がある.（ウ）を防ぐために出力値の分布の偏りを抑制する（エ）が 2015 年に提案されている.
正解は3である。
DistBeliefはGoogleが開発した深層分散学習のフレームワークである。これは論文も出されていて、深層分散学習の仕組みを理解できる。
Hadoop
選択肢1: 分散技術を用いたアプリケーションである。
ReNet
選択肢2: ResNetはCNNを用いた層の深い画像分類器である。
DistBelief
選択肢3: 上述の通りである。
MapReduce
選択肢4: 並列処理を行うためのプログラミングモデルである。



周りの値をp乗しその標準偏差をとるLpプーリングなどが存在する。



ハイパーパラメータを含め最適化問題とする（ウ）が効率的なチューニング方法として注目をあびている
ベイズ最適化


ファインチューニング
既存のモデルの一部を利用して新たなモデルを解くために再学習する手法である



蒸留はモデルのパラメータを小さくする手法の一つである

ディープラーニング
人工知能研究における50年来のブレイクスルー


1996年 IBMのdeep blueがチェスの世界チャンピオン　ガルリ・カスパロフに勝利

2012年Bonkras将棋電王戦で勝つ　現在はPuella αと名前を変える
2013年Ponanza　山本一成が開発 将棋ソフト


SharpChess　チェスソフトウェア


AGI artificial general intelligence 汎用人工知能　強いAIに近いもの



正答率　全体に対して正解サンプル数の比

適合率　precision

真に正解
真に正解+間違って正解と予測
の比



確率的勾配降下法で1サンプルづつ使う方法をオンライン学習という。
全体を使うとバッチ学習
少数の群を使うとミニバッチ学習



時系列データにはRNNが適しているが、音声処理にはautoencoderが適していたりもする
JDLA公式模擬問題ではCNNとRNNの順になっていた。謎。



ElmanNet は、RNN の一種でありJordanNet と並んで初期の有力なネットワーク構造の一つである。

ResNet は、Residual Network の略称で、当時 Microsoft Research の Kaiming He 氏によって2015年に考案されたニューラルネットワークのモデル。
ある層で求める最適な出力を学習するのではなく、層の入力を参照した残差関数を学習することで、それ以前に比べて圧倒的に多い数の層のネットワークを構築することに成功している。
2015年にImageNetコンペで一位になった。
152層
ショートカットするネットワーク構造


5. LeNetは、CNNの黎明期のモデルです。ディープラーニング研究の大家、Yann LeCun氏によって1988年に発表されました。
1. AlexNet は、トロント大学のヒントン教授らのチームが、 ILSVRC にて他のチームを圧倒する正答率を叩き出したCNNのモデルである。
3. GoogLeNetは、インセプションモジュールと呼ばれる構造を内包しているCNNのモデルの一種で、ILSVRC-2014 の分類問題において最も高い精度を叩き出して優勝したネットワークのことです。
22層
7. VGG16とは、2014年のILSVRCのクラス分類部門で、GoogLeNetに劣らず高い精度を叩き出したモデルで、問題文にもある通り、University of Oxford の Visual Geometry Group という研究グループにより提出されたモデルである。
16層


8.WaveNet とは、DeepMind のリサーチャーたちによって開発された、音声波形を生成するためのニューラルネットワークの一つである。これによって、自然な発話の再現へ向け大きな進歩が生まれたとされている。 

ImageNet は、大規模でかつ良質なデータセットを公開している画像データベースのプラットフォームのことを指します。
MNIST 手書き画像のデータセット



インセプションモジュール
小さなネットワークをひとつのモジュールとして組み合わせて使う。
googLeNetがコレを使っている


RNN
入力データに加えて過去の中間層の状態を隠れ層に取り込むという再帰構造を取り入れたネットワークになっている


複数のセンサーから得られた別々の情報を統合して処理するシステムのことをマルチモーダルシステムと呼ぶ

コグニティブ
推論、信念、考えや知識などの認識にまつわるシステムのことすべてを指す一般的な用語、もしくは人間の知的能力をどうにか模倣して新たなソリューションを生み出すためのシステムのことを指す。 




人工知能
1956年ダートマス会議
ジョンマッカーシーが言った言葉
ダートマス会議ではニューウェルとサイモンが世界初の人工知能プログラムの
「ロジック・セオリスト」を使って数学の定理を証明した。
四則演算しかできない機械という認識が覆された。

推論、認識、判断など人間とおなじ知的な処理能力を持つ機械(情報処理システム)であると専門家たちは共通認識している


アーサー・サミュエルは
明示的にプログラムしなくても学習する能力をコンピュータに与える研究分野としている
(少し古い定義)
チェッカーゲーム(チェスみたいなゲーム)

トム・ミッチェル
課題T
性能基準P
経験Eと共にTの性能がPで測った時に向上している時にEから学習したと言える。
機械学習・学習の定義


AI効果　AIに詳しくなるとAIが知能ではなく自動化として認識しはじめる心理効果

1946年エニアックENIAC
アメリカ　ペンシルベニア大学
世界初の汎用電子コンピュータ

第一次AIブーム
1950-1960
推論と探索の研究が進む
迷路や定理の証明など限定された範囲での簡単な問題、トイプロブレムは解ける
複雑なものが解けずに世間が冷める

第二次
1980年
専門家の知識を大量に入れるエキスパートシステムが主題
医療診断・翻訳・故障診断について専門家の知識で振り分けていくシステムは、次第に入力する知識の方が多く現実的でなくなっていった
知識を入力し、システムを構築する技術者をナレッジエンジニアと呼ぶ。
日本では第五世代コンピュータの時代
1995年に冷める

第三次
2010年から
機械の性能が向上、ビックデータを扱えるようになった
機械学習が実用化し、特に特徴量を自ら選択するディープラーニングが登場したことでブームになる
ディープラーニングの処理過程で自動的に特徴量を抽出することから、時に特徴表現学習とも呼ばれる。
画像認識コンペ・alphaGoから知名度が上昇
レイ・カーツワイルがシンギュラリティを懸念しはじめる
alphaGoは2015年google deep mind社で開発された。
deep mind社は2010年設立、2014年にgoogleに買収された

バンドワゴン効果　行列に並ぶ　モテればモテる　多数派に流れる
ELIZA効果　いらいざ　意識的に機械と理解していてもコンピュータの動作を人間っぽいと感じてしまうこと。
ピグマリオン効果　　ほめられると伸びる　別名　教師期待効果、ローゼンタール効果



迷路を解く探査木
幅優先探索　ゴールまで一番最短距離の経路を見つける。深くなればなるほどメモリ不足が心配。
深さ優先探索　一本一本一番深くまで探す。時間がかかる。ゴール方法が複数ある場合、最初に見つかったものが最短距離のゴールでない場合もある。


プランニング　ロボットに　前提条件・行動・結果を明示させておき、目標状態までの行動計画を立てさせる。
この3つで記述する方法をSTRIPSと呼ぶ。
スタンフォードのテリーウィノグラードさんは、プランニングをコンピュータの中の「積み木の世界」で研究した。
積み木の世界に対するシステムをSHRDLUと呼ぶ。
Cycプロジェクトにもつながる

ボードゲームを解くAI
min-max法　自分手の時に最大報酬となる手を、相手のターンは最小になるような手を考えていく方法
自分の手を考える時に、報酬の小さい方法はそもそも探索対象から外してしまうことを、βカットという。
相手が打った手に対して、打たなかった手の場合の探索をやめてしまうことをαカットという。

モンテカルロ法
ある程度の局面まで探索で進め、決まった局面まで進んだらプレイアウト法によってランダムに手を指した場合のシミュレーションを行い、勝ちパターンにつながりやすい手を決める。
この方法は、人間が盤面を見て付けた報酬が正しくない場合に有効であり、とにかく手を打って終局まで計算し、勝ちにつながった手を選択するというプレイができる。
この方法は力任せ(ブルートフォース)と呼ばれる。


イライザ
1966年ジョゼフ・ワイゼンバウムが開発
発言に対して決まった返事をするプログラム。
精神科セラピストの役割を演じている。
ただのオウム返しや定型文の返事でも人間と話しているように感じる、イライザ効果

マイシン(MYCIN)
1970年　スタンフォード大学で開発
血液中のバクテリアを診断支援するルールベースプログラム。


DENDRAL
スタンフォード大学
1960年エドワード・ファイ現場有無
未知の有機化合物を特定するプログラム

これらのエキスパートシステムに知識を入力するのが大変。
インタビューシステムなどが登場。

意味ネットワーク semantic network
言葉の関係をis-aやpart-ofで結ぶ
矢印で上位概念などを表すことができる。


cycプロジェクト
一般常識を機械に入力しつづける研究分野

オントロジー
人によって認識や記述方法などが異なるが、これを統一化しようと始まった分野。
決まった方法で知識を記入することで、同じ知識については誰が書いても同じものとなる。
is-aやpart-ofもオントロジーの概念関係をあらわすもの。
オントロジーで記入された言葉の関係を使ってウェブデータを解析したり、文書データを解析したりすることが出来るようになった。
セマンティックwebなどで注目されている。


ワトソン
2011年のアメリカのクイズ番組で人に勝った
IBM製
ウィキペディア情報から関連しそうな答えを探索している。

東ロボくん
東大に合格するために頑張っているロボット
偏差値57.8まで頑張った
質問の意味を理解していないため精度が上がりにくく2016年で一旦止まる。


フレーム問題　1969年ジョン・マッカーシーとパトリック・ヘイズが提唱
関係のある事柄だけを抽出するのは難しい

チューリングテスト
会話相手がコンピュータか人間か判定できるかどうか

強いAI弱いAI
1980年ジョン・サール
弱いAIは道具
強いAIは心を持つコンピュータ
中国語の部屋という思考実験を通して、知能を持っているようにふるまえても本当に知能を持っているかは分からないと発言。



シンボルグラウンディング問題
スティーブン・ハルナッドさんが言っている。
シマウマというものを、シマとウマの言葉から結び付けるのはコンピュータにとって難しい




知識獲得のボトルネック
全ての一般常識を取り込んで判断しないと正しい判断ができない。
望遠鏡を持っているのは男性の場合が多い、など。
翻訳機を作るときには大切だが、どうやって入れ込むかが大変



宇宙物理　スティーブン・ホーキング　完全なAIの完成は人類の終焉を意味する
イーロン・マスク　慎重に。悪魔を呼び出している。
ビル・ゲイツ　人工知能に懸念を抱く側の一人



レイ・カーツワイルのシンギュラリティ
人工知能が自分より賢い人工知能を作り出す
知的なシステムの技術開発速度が無限大になる特異点である
その以前に人間よりも人工知能が賢くなるのは2029年と予測。





GPU　画像処理
CPU　順番に処理
GPGPU　画像処理以外にも使うGPU NVIDIA社
TPU　テンソル行列計算に使う google社

バーニーおじさんのルール　モデルのパラメータ数の10倍のデータが必要　という機械学習モデルとデータ量の経験則的な関係

CNN画像をそのまま入力として使える。
前駆となったのは1998年のヤン・ルカンのLeNet


2006年　トロント大ジェフリーヒントン
オートエンコーダー(自己符号化器)を積み重ねて精度を上げようとした。
積層オートエンコーダー(stacked autoencoder)
全体を一気に学習させるのでなく、一層ずつ学習させてから(事前学習(pre-training))全体を最適化する(ファインチューニングfine-tuning)で積層エンコーダにアプローチ。
事前学習を挟むことでニューラルネットワークが深くなると学習できない問題を突破した。
同時期に深層信念ネットワーク(deep belief)という制限付きボルツマンマシンによる教師なし学習へのアプローチを提案していた


ImageNet Large Scale Visual Recognition Challenge（ILSVRC） 2012
トロント大学のジェフリー・ヒントン率いるSuperVisionチームが Alex netで優勝
チームメンバーのalexの名前を取って命名したCNN。
畳み込み、プーリング、畳み込み、、、という構造。
GoogLeNetやVGGもこれに続いて出だした。
他のチームがエラー率26％前後のところ、エラー率17％弱とダントツの認識率をマーク
同じく2012年、米Googleがディープラーニングで構成された人工知能にYouTubeの画像を見せ続け学習させた結果、猫の画像を猫と認識できるようになった

ディープラーニングフレームワーク
カリフォルニア大学バークレー校による「Caffe」
トロント大学のSuperVisionのメンバーであるAlex Krizhevsky氏が公開している「Cuda convnet2」

R-CNN
regional-CNN
領域とする四角形を回帰問題として推定する問題を解いている。
まず位置を見つけ、その中に何が入っているかを見つける。
領域を示す四角形をバンディングボックスという。
fastRCNNやfasterRCNN,YOLO,SSDといったモデルが次々に考案されて来ている


セマンティックセグメンテーション
semantic segmentation
四角形での物体認識ではなく、画素単位での検出ができる。
輪郭にフィットして分けることが出来る。
全てを畳み込み層で構築したネットワークをFCNと呼ぶ


セマンティックやバウンディングボックスによって物を背景と切り離せる。
この切り離した後の物にラベルをつけるのがインスタンスセグメンテーションである。

トマス・ミコロフ　word2vecを提案。fastTextも2013年に提案。
word2vecから単語埋め込みモデルが爆発的に発展し、後続のfastTextとELMoが生まれた
word2vecのおかげで類似語を計算したり、単語同士を引き算したりできるようになった。
fastTextでは、登場していない単語OOVも表現できるようになった。


NIC　ニューラル画像脚注付け
画像認識と言語モデルを組み合わせて画像にキャプションをつける研究分野


deep blue 
ディープ・ブルー
IBM社のチェスロボット　前駆の研究にディープ・ソートがある。　人間のプロに勝ってる


アーサー・サミュエル
機械学習とは、
明示的にプログラムしなくても学習する能力をコンピュータに与える研究分野








アラン・チューリング　AIとか以前にコンピュータの基礎を考えた人


ダートマス会議 1956年　米国ニューハンプシャーの研究集会

第一次AIブーム 1950-1960 探索と推論　ルールのあるゲームの中では問題解決できる。探索の効率化が研究課題とされていた。組み合わせ爆発を防ぐ方法など。

第二次AIブーム 1970-1980　知識は力なり　エキスパートシステム　ルール型　専門家へのヒアリングをルールとして書き下していったもの。AIとして大規模にやるには労力が追い付かない。
1980 機械学習・データマイニング活動が活発になる
















AIのInteligenceの部分、知識には二通りある
記号化された知識とその活用--抽象化した概念をうまく使うにはどうしたらいいか？？
知覚によるパターン認識--自ら知覚したものを抽象化すること。



人口ニューラルネットワーク　ANN 1943年 ウォーレン・マカロック(外科医)　ウォルター・ピッツ(数学)　形式ニューロン　formal neuron　もしくは　 Threshold Logic Unit　パーセプトロンの元になった。


三層パーセプトロン　1958 フランク・ローゼンブラット　入力・中間・出力の三層　まだこのころは逆誤差伝播が学習のためにつかわれてはいなかった。ジェフェリー・ヒントンはこのころから考えていたらしい


ビックデータ時代　1990-2000　web拡大、IoT,CPS(cyber physical system)などの物理空間とサイバー空間の結びつき、が顕著となった

第三次AIブーム ニューラルネットの活躍　2012　DNNはデータサイエンスの範囲ではなくAIの範囲　統計手法活用者はデータサイエンティストと呼ばれる　AIもデータサイエンスもパターン認識・識別の技術を持っている点で共通している
データは新しい石油、AIの燃料はデータ



ビックデータの性質3V
volume　量
velocity　生成・収集・分析速度の向上
variety 非構造化データ

世界データの70-80%は企業のプライベートデータである。


2016年12月　官民データ活用推進基本法　国公共団体はデータオープン化の取り組みを義務づける

2017年5月　改正個人情報保護法　個人情報の含まれるデータ利用方法について定めている

2017年4月-2018年5月　次世代医療基盤法　医療情報活用のため、データを第三者提供できるようになった

2018年5月　一般データ保護規則 GDPR 　EUの個人情報保護、かなり厳しいらしい。個人情報の収集は必ず同意して集めるとかそんな内容いろいろ。

GAFA google apple facebook amazon 米国の国際的プラットフォーマー












2018年　経済産業省　産業データ活用促進事業　データの共有共用を可能にする基盤構築と実運用の実現を目指す。
民間では　一般社団法人データ流通促進協議会で話し合われている


情報銀行　個人の情報を必要とする企業に仲介・提供する機関　考えられているところらしい

MyData 個人が持つ情報は個人でコントロールできる、公開などもできるし、それで見返りを求めることもできる　という思想

Personal Data Storage PDS 情報銀行は個人から提供されたPDSを流通させる



















		
world models　世界モデル	運動系の強化学習に使われる。シミュレーション空間？	
controller	強化学習のタスク実行を行う小規模なモデル空間？もしくはそのタスク	
deep mind	英国のベンチャー　googleに買収された　demis hassabis デミス・ハサビスさん	
GQN	generativi query network 二次元画像から三次元画像を作り出す技術。deep mindが発表	
SLAM	simultaneously localization and mapping カメラ、センサを使った空間・立体認識技術	
Mobileye モビールアイ	イスラエルの会社　自動車のカメラ　衝突補助　intelに買収される　リアルタイムで道路状況をサーバへ挙げていた点が付加価値となる	
mary meeker　メアリー・ミーカー	元証券アナリスト　投資家　インターネット　トレンドレポート　分析レポートが人気な人	
Ian goodfellow	GANの開発者　日本で「深層学習」って本出している人	
GANs	Generative adversarial networks 敵対的生成ネットワーク　教師なし学習　深層学習の一つ	
消費インテリジェンス	消費のデータから消費者理解を促進する能力を示す概念。　消費者データの分析から戦略的活用ができる人材を増やしていくのがこれから大切	
			











第三次AIブーム	
ディープラーニングによって引き起こされた	
	
ILSVRC	image net large scale visual recognition challenge	
2012年 トロント大学のジェフリーヒントンの研究室が大勝	
人間の精度を上回る認識力によって、自動化・機械化の可能性が出てきたことから「AIが眼をもった」といわれる	
			
シンボルグラウンディング問題	
実体の概念と　実体についている名前　が結び付けることは機械には難しい問題。　シマウマの例が有名	
グラウンドした記号	記号と実体が　結び付けられた記号の事	
			
勾配降下	
微分の連鎖律によってディープニューラルネットの最適化を求めるアルゴリズムのこと。	
ディープラーニングは深い層の関数の最小二乗法にすぎない	
最小二乗法は全データを使った線形代数の問題なので計算に時間がかかる
勾配降下法で言うバッチ学習に相当する。
一部のデータを使った勾配降下法が良く使われる
データ一つとってきて勾配計算をするものをオンライン学習という
データを数個まとめてとってくるものをミニバッチ学習という
ミニバッチの個数が一個ならオンライン学習と同じだし、ミニバッチが全データならバッチ学習におなじ　



			
ヤコビ行列	
行列版の微分のこと
			
連鎖律	
合成関数を微分した導関数は、それぞれの導関数の積として表せるという関係性のこと。
確率は独立なら積のカタチに分解できるということ
			
			
深い関数
ディープニューラルネットのこと。　深いほど表現力が上がる	
			
教師あり学習	
正解の値の入った列を教師データと呼ぶ。	
機械学習では元のすべてのデータを何割かに分割して訓練データとよぶ	
もう一方をテストデータと呼ぶ			
訓練データで学習(予測器の構築)を行い、テストデータで、未知のデータにうまく使えるかを試す	
			
過剰適合	
訓練データのみ正確に予測できてテストデータでは予測できなくなってしまった状態	
あるいは過学習・オーバーフィットなどとも呼ばれる


			
正則化	
過学習を防ぐために行われる数学的な処理	
AI中にあるムダに敏感な部分を数学的に消してやることで、訓練データへの過剰な適合を防ぐ	
スパースにするものがLASSOと呼ばれる。L1正則化とも呼ばれ、L1ノルムをペナルティに使う。
L2正則化と呼ばれるものもあり、Ridgeと呼ばれる。重み減衰・L2ノルム・チコノフ正則化などが出てきたらこっち。スパースではない。
どちらの特徴もいい感じに取り入れたものがElastic Netと呼ばれる



生成モデル
VAE 変分オートエンコーダー
GAN　敵対的生成ネットワーク　生成ネットワークと識別ネットワークを戦わせる。　「識別」の部分がテストで空欄になる印象
			
	

汎化性能	
未知のデータ、確認時にはテストデータ、に対してもうまく予測ができる能力。過剰適合していないAIを、汎化性能を持ったAIと呼ぶ	
			
	
教師なし学習	
教師データがない。なので学習して予測する先がない。	
学習の対象は予測というよりもパターンを見つけることが多い。	
全て正常と考えて異常を見つける、集団をまとまりのいいグループに分ける、など。	
クラスタリング・次元削減・素性学習・密度推定などなど。	
GAN、マハラノビス、k-meansなど


			



オートエンコーダー	
入力したものをそのまま出力する　ニューラルネット	
教師なし学習に分類される（wikiでは教師あり学習と言われている）	
入力データをニューラルネットに入れ、出力が入力データと同じになるように学習させる。	
三層のオートエンコーダは次元削減と同じ効果を発揮する。三層の中間層に特徴が抽出される。
2006年　ジェフリーヒントンが提案していた。


			
ニューラルネット	
人間の神経細胞（ニューロン）を計算器上で再現したもの。	
			
	
強化学習	
学習を行う主体のことをエージェントと呼ぶ	
エージェントの、「状態」と、次の選択した「行動」をもとに、「次の状態」に移ったと考え、この間の情報を評価する「報酬」ことによって、	
報酬が最大になるような行動を選ぶようになる。	
最大化を図る、測った行動方針のことを方策という	
迷路問題ではマスに状態の価値が付けられる。	
価値反復法というアルゴリズムを使うと周囲のマスの値からマスに価値を付けられる。			
現実データでは迷路のような制限されたマスでなく、変数の空間一つ一つに価値を結び付けるひつようがあり、これを深層学習で割り当てることで精度が良くなった	
			
			



	画像・音声など、生データであり、ドメイン知識を知っていても特徴量を抽出することが比較的難しい処理にディープラーニングは向いている。		
	ディープラーニングは表現力が高いから。		
	マシンラーニング系はドメイン知識を持った人が入ることが多い。ディープラーニングも使われる。		
	ディープラーニングだからといってドメイン知識なしの人が不要ということはない。		
	逆に言うと、かなり本質を表している特徴量を持ったデータであれば、ディープにする必要は無いし、既存の統計的機械学習に任せたほうが精度が高いこともある		
	人間の介在なしに特徴量を抽出できる点がディープラーニングのすごいところである		
						
	ディープラーニングにはデータ量に対応してパフォーマンスが上がるという特徴がある。		
	そのため、データ収集方法や、データの正解をつくるデータのアノテーション作業などがプロジェクトのボトルネックとなることが多い。		
			
知識獲得のボトルネック
人間の知識をもとに判別させようとすると知識を入力するのが大変すぎて間に合わない
専門知識を入力するナレッジエンジニアとかいう職業もできたけど、それでも大変すぎて追いつかない



end-to-end学習
ディープラーニングは途中にドメイン知識を持った人が特徴量操作などを行わずとも正解クラスにうまく割り振ってくれる特徴がある。			
ディープラーニングは特徴量を自ら見つけてきてくれているとよく言われる	

	
特徴量表現学習	
ディープラーニングが学習の過程で特徴量を自らみつけてくれることから、そう呼ばれる。	
単に表現学習ともいわれる。表現(特徴量)が人間にとって分かりやすいかは別のはなし。	
			
			
	
深層強化学習	
ディープラーニングによって得られた特徴量を使って、状態を定義していくことで強化学習がうまくいっている。			
例えば碁盤の画像を深層学習で認識させ、これを特徴量に変換し、状態としてやることで、alphagoの性能につながっている	
alpha goでは、さらに先読みを探索によって行うことで成果を上げている	
			
BRETT	
バークレーの開発したロボット。ディープラーニングと強化学習を組み合わせている	
タスクに応じて報酬を設定すれば、さまざまな動作を同じプログラムに学ばせることができる	
学習結果はコピーもできる	
ブレットと読む
			
	
生成モデル	
generrative model	
オートエンコーダーも変分オートエンコーダーも識別モデルに分類される	
オートエンコーダーは生成モデルではない。しかし画像生成はできる。	
白黒写真に色を付ける
簡単なイラストを本格的に変換してくれる
部屋の画像を学習させておくと、存在しない部屋のイラストを創り出してくれる
データの生成過程をモデル化するAIのこと	
入力されたデータyをもとに、一番出力しそうな結果xを出力しているだけ。同時確率を求めている	
P(x|y)P(y)	ナイーブベイズみたい	
グラフィカルモデルのような確率変数間の依存関係を表したモデルで実現されてきた	
GANやVAEがコレ。オートエンコーダも一応生成できたりするみたいだけどテストで厄介になるのでいったん忘れる。



			
変分オートエンコーダー	
深層生成モデルのひとつ	
教師なし学習	
variational autoencoder	
VAE	https://www.youtube.com/watch?v=OLm5498MGb0
入力に対して、確率的に出力が決まる	
出力は確率分布のパラメータらしいが・・・・	https://tips-memo.com/vae-pytorch
パラメータによって画像の変化を表現できる。笑っている～泣いている　顔の確率分布のパラメータを見つけられる	
			
GAN	
generative adversarial network	
生成的敵対ネットワーク	
深層生成モデルの一つ	
精巧な偽物をつくる技術	
			
識別モデル	
猫の画像からネコと判断できるモデルのこと	
ロジスティック回帰、SVMなど	
			

CNN	
convolutional neural network	
画像は近くの画素から影響を受けているので、ピクセルデータを並べるよりも、周囲をまとめて切り取ってデータとしてやったほうがいい。という考えでできた	
CNNは入力に近い部分では簡単な特徴量が抽出され、層を重ねると複雑な特徴量が学習されることになる。	
alex net	
VGG-16	
VGG-19	
Inception(GoogLe netのこと。インセプション構造を持っているから。)
Resnet	
などが有名なモデル	
			
convolution	
畳み込み	
信号処理なんかでは一般的な用語。画像の畳み込みでなく、信号処理の畳み込みを見ておくといいかも。
フィルタと呼ばれる関数を小さな領域に重ね合わせて、素性マップを生成する行為	
フィルタによる数値変換のこと	
		




素性　特徴量のこと
素性エンジニアリング　効果的な特徴量をつくること。ディープラーニングが結構担当してくれるようになったけどまだ大切









	
素性マップ	
CNNのようなモデルでフィルタによって抽出された結果。画像はデータが二次元なので　マップ　と言われる。	
feature 素性 特徴量	
			
			
pooling	プーリング	
素性マップに含まれる素性のプール(かたまり)を作り、最も大きいものを取り出したり、素性マップの平均を取り出したりする行為	
領域を集約すること	
			
padding	パディング	
畳み込み時に、画像のすみっこの情報が欠損してしまうので、余白を作って情報損失をすくなくする試み。	
余白に相当する値を0でうめることをゼロパディングという	
そもそもパディングは詰め物とか水増しという英単語で、IT用語ではデータの長さを一定にするために無意味なデータを詰めることに使われる単語。	
			
ReLU	
Rectified Linear Unit	
活性化関数の一つ	
データを非線形変換するための関数	
勾配消失問題の解決になった。シグモイド関数では勾配が限りなく0になってしまう点が出るため、学習が進まない。	
			
			
CNNの手順	
畳み込み層の後に活性化関数をはさむ			
プーリングで小さくなる	
フラット化して全結合にする	
softmaxで出力を確率に見えるように正規化してクラスに当てはめられるようにする	
			
時系列データの深層学習	
近接した要素は次の値に影響を与える可能性が高く、遠いものは可能性が低い。	
この考えから、全データを使わず、周辺のデータに注目することでパラメータ数を減らしたものがRNNとよばれる	
			
RNN	
Recurrent Neural Network	
翻訳・時系列・画像認識と組み合わせられている	
中身を見てみると、文の長さ、文章の種類など、特徴量が自動的に抽出されていることが確認できる	
しかしRNNでは短期間の周辺データしか確認できない	
			
LSTM	
long short term memory	
過去の状態を記憶しておくメモリセルをつくることで、長い期間の時系列を扱えるようになった。	
メモリセルは忘却ゲートを使ってリセットされる。	
			

seq2seq	
入力を一旦すべて特徴量に変換してから、他言語や画像などのクラスに対応するようにデコードしていると考えることが出来る。	
例えば、画像にキャプションを付ける場合には、キャプション文字の特徴量を、画像のクラスに写像している	
畳み込みによって特徴抽出したデータをそのままLSTMなどにぶち込むことで実現する
			
SMT	
統計的機械翻訳　
statistical machine translation	
NMT(ニューラル機械翻訳)以前のもの。NMTのほうが性能がいい	
統計的機械翻訳モデル　
翻訳前の単語が、翻訳されるべき単語と一致しているかの確率を計算する。次に、翻訳単語を繋げて作った文が、元の文の意味と一致しているかを確認する。という二段構成のモデル。前者を言語モデル、後者は翻訳モデルと呼ばれる。
深層学習によって精度が向上している。なかでもseq2seqモデルが使われることが多い。



NMT	
ニューラル機械翻訳　Neural machine translation	
時系列（sequence）から時系列を予測するモデルをseq2seqと呼ぶ	
入力を時系列として、出力も時系列とすると、機械翻訳ができる。これをNMTと呼ぶ。	



			
DQN	
深層強化学習の一種 Deep Q network	
Q学習を深くしたもの。	
deep mindがつくった	
Q学習の行動価値関数をneural netで置き換えたもの。	
状態sからの行動aによる価値Q(s,a)を、画像から求めることでゲーム中の行動を学習させ、人間に勝った	
			
GQN generative query network  deep mind社の二次元画像から三次元画像をつくる技術
		
Q学習	
状態価値関数Vと、行動価値関数Qが登場する	
状態から将来得られる価値を計算するVと、状態から行動をすることの報酬を計算するQ	
状態Xから、得られる報酬Yを求める回帰問題になる	
			
身体性	
身体性を持たない知能の限界としてSHRDLUがある	
体があるから、外部の環境から特徴量を得られているという発想がある	
本当の意味理解につながるかもしれない	



			
SHRDLU	
積み木の世界	
テリー・ウィノグラードによるシミュレーション	
terry winograd	
積み木の形状や、どの積み木の上にどのブロックのどの面が置けて、ブロックの面積は・・・	
というおぜん立てが必要だった	
GANはおぜん立てなしで騙されまいと学習していく。	
最近の生成モデルというやつらは自らデータを生成しつつ、経験的に学習していく。そのためおぜん立てなしで意味理解をしていくことができるのですごい！	






工学的なパターン認識にネットワークを使った最初の試みがネオコグニトロン
1980年代に福島邦彦
多層化された人工ニューラルネットワーク
「単純細胞（英語版）」および「複雑細胞（英語版）」と呼ばれる一次視覚野の2種類の細胞を発見し、パターン認識タスクにおいて使用されるこれら2種類の細胞のカスケードモデルを提唱した



畳み込みネットの直接の元になったものはLeNet
似た構造を持っていてﾈｺの視神経から着想を得たネオコグニトロンと混ざらないように注意。



elmanネット johdanネット エコー状態(エコーステートネットESN)　時間遅れNN 
これらすべてRNN。









RNNの確率的勾配降下法
RTRL法　メモリ効率がいい
BPTT法　速度が速くシンプルな構造　勾配消失が出てくるからメモリーセルという構造を入れたLSTMが作られた

RNNは勾配消失や爆発問題を抱えており、長い時刻分は動くことができない
そこでLSTMが考えられた
過去のデータを保存しておく構造を持っている


















P(は|こんにち)の確率は高い　
確率によって言葉の流れをモデル化するものを　言語モデル　という

深層学習がはいった言語モデルを
ニューラル言語モデル


決定着モデル
不純度の減少量を最大化する=情報利得の最大化
変数を事前にスケールしておく必要がない。というメリットがある





K-NN 
未知データとちかいデータの正解ラベルを参照し、多数決をとる
参照データ数をkとして設定
データのラベルに偏りがあると精度が落ちるという問題あり



交差検証
教師データが少なくても安定して精度を測定できる



局所・大域・停留点(鞍点・変曲点を含む)


畳み込みは順伝播型のニューラルネット　実は時系列にも使われている





GAN　敵対的生成ネットワーク
教師なし学習に用いられる
イアン・グッドフェローが考案

生成側が生成したデータか、本当にあるデータ(一応教師データ)かを見極める　識別ネットワークがいる
ヤンルカンがここ10年で最も面白いアイデアと形容した

WaveNetはdeep mind社のだしたもの　2016年
音声合成をする深層学習モデルである
音声を時系列と見なして畳み込み処理を行っている。


クラシックな音声認識の知識
文章から余分な文字列を消す
形態素解析で単語に分ける
構文解析して、意味解析して、文脈解析する


最近よく出てくる自然言語処理(NLP)の流れ
形態素解析(すもももももももものうち　を単語に分ける)
N-gram Model N語に区切ってモデル化する　すも　なら　N=2 すもも　ならN=3
単語を辞書のようなものにする(BoW)bag of word
こんなものを深層学習の入力としてぶちこむ







bag-of-words ベクトルの各要素に単語を対応させる
単語分解して列に直す。ワンホット表現で1と0が入る。
登場回数が入るときもある。
ベクトル分類器のSVMなんかを使って文書をベクトルとして分類することができる。


bag-of-n-grams 単語の登場の順番パターンを見るもの。bag-of-wordsだと一単語を単語リストにするが、n=2だと2単語をリストにする。よく続けて登場するような単語のくみあわせが分かる。nは何単語のつながりまで見るかを指定する。nは試験的にいろいろと試してうまくいくものを探す。


評価文書分類　良い悪い普通などの文章の持っている評価レベルに分類する話。

構造解析　固有表現かどうかを周囲の単語から解析する。依存関係の分析もできる。「アプリが圏外ではフリーズします」という言葉のアプリには、何が関係を持っているのか、アプリが圏外なのか、アプリがフリーズするのかをわかるようにする。
日本語では係り受けという。cabochaやKNPといった解析ライブラリが日本ではある。
英語には文節という概念がないため、単語間の関係性を分析するstanford parserというツールが使われる。
















word2vec　
は周辺語の予測というタスクを解く過程で得られる。
周辺語の予測をskip-gram modelという手法で解く
kingとかqueensの単語を矢印をもったベクトル表現にすること
単語分散表現を可能にした。
king - men + women = queen とか計算できるようになった 








Tay 
マイクロソフト社のツイッターでのおしゃべりbot
不適切な教育をされて不適切な発言をするようになった


LAWS 自律型致死性兵器
自律で殺傷能力をもつ兵器
現在では存在していない。
民間人や味方を殺す可能性あり
2017年2月　アシロマAI原則　で　AIによる軍拡競争はさけるべき　と明示
Future of Life Institute（FLI）が出している原則
23の原則がきめられたガイドライン


ディープラーニングのノードを消すと過学習を防ぐ
dropout



バッチ正則化
シグモイド関数を活性化関数に使うと、微分すると0になってしまうようなところに入り込んでしまう。
すると活性化関数の出力が一定のまま変化しなくなってしまう。
そこで学習している層の出力を正規化してやることで学習をすすめることができる。
シグモイドに限らずReLUを含んだネットワークで使ったって問題ない。


データ拡張
学習データが少ないときにデータに変換をかけたものを作り、元のデータとともに学習につかう。
画像認識のネットワークを作りたいときに


転移学習
すでに誰かが作ったモデルを似ているタスクにそのままつかわせてもらう事
自分のタスクに合うように層を追加したり構造の一部をリセットしたりして学習しなおすことをファインチューニングという


蒸留
学習済みモデルの入力と出力を使って、新しいネットワークに学習させる
転移学習というよりは学習済みモデルの変換方式をまねするような新しいネットワークを作ること
計算量が少なくていいモデルができるらしい



バイアス・バリアンス問題
学習不足の時、高バイアス・低バリアンスという
過学習している時、高バリアンス・低バイアスの状態という
この二つはトレードオフの関係であり、難しい問題。


ランダムフォレストはバギングの手法
XGboostはブースティングの手法








初期化方法
Glorot
Xavier
He









ReLU 正規化線形関数　Rectified Linear Unit 

Maxout 領域ごとに線形の傾きが変わる

双曲線正接関数　Tanh関数とも呼ばれる。Tanh関数よりもReLU関数の方が勾配消失問題が起こりづらい。



ドロップアウト　ニューラルネットの過学習を防ぐため一部のノードを無効化する



L2正則化　ノルムにペナルティを課す


ResNetは画像分類タスクで用いられるCNNモデルである.

VGG16は画像分類タスクで用いられるCNNモデルである.

VAEは平均や分散などを求める生成モデルである.
ディープラーニングのモデルの一つ、Variational Autoencoder(VAE)
訓練データを元にその特徴を捉えて訓練データセットに似たデータを生成することができます
VAEはこの潜在変数zzに確率分布、通常z∼N(0,1)z∼N(0,1)を仮定したところが大きな違いです
自己符号化器の潜在変数に確率分布を導入した




GANは生成ネットワーク,識別ネットワークの二つを競い合わせることで生成モデルを獲得する.
画像生成分野でよく使用される.







マルチタスク学習　同時に複数の識別問題に対応できるように学習する手法のことである.


強化学習は行動を学習する手法のことである.

アンサンブル学習は複数の学習器を組み合わせて予測する手法のことである.







正則化　訓練誤差ではなく汎化誤差を可能なかぎり小さくする手法である正則化を用いる








バイアスは実際値と予測値との誤差の平均のこと
バリアンスは予測値がどれだけ散らばっているかを示す度合い


バギングとは「Bootstrap Aggregating」の略で一般的にモデルの予測結果のバリアンスを低くする特徴があります。アンサンブル学習。
バギングではブートストラップ手法を用いて学習データを復元抽出することによってデータセットに多様性を持たせています。復元抽出とは、一度抽出したサンプルが再び抽出の対象になるような抽出方法です。

ブースティングは一般的にモデルの予測精度に対してバイアスを下げる特徴があります。
間違った予測に焦点を当てて「重み」を加味して次のモデルを改善していくのです。



スタッキング
スタッキングとは言葉の通りモデルを積み上げていく方法です。上手く利用することによりバイアスとバリアンスをバランスよく調整する事が可能です



Lasso　パラメータをスパースにする

正則化にはL1正則化であるLasso正則化と、L2正則化であるRidge正則化の二種類が存在する。L1正則化を使用するとスパースになる。

Elastic NetはLasso正則化とRidge正則化の中間である。









ノーフリーランチ定理　組み合わせ最適化の定理である。

非線形分離問題　非線形になると分離できない問題のことであり、単純なパーセプトロンで起こる。

事前学習　勾配消失問題はモデルのパラメータの初期値に対して依存する。モデルを事前学習させることで安定した学習が可能となり、勾配消失問題を克服できると考えられる。






ステップ関数　微分値は常に0なので、ほとんどのニューラルネットワークでは使用されない。


DistBeliefはGoogleが開発した深層分散学習のフレームワーク
パラメータ数が膨大となるため,処理の高速化が必要となる.2012 年に提案された分散並列技術



Hadoop　分散技術を用いたアプリケーション

MapReduce　並列処理を行うためのプログラミングモデル


GPU　画像処理に特化したプロセッサ

InfniBand　非常に高い信頼度のクラスタ用のアーキテクチャである


FPGA　ハード回路のことであり、ディープラーニングの学習には使用しない。




内部共変量シフト　
大規模なニューラルネットワークを学習すると、ある層の入力がそれより下層の学習が進むにつれて変化してしまうことがある。
これにより学習が止まってしまうことが考えられる。このことを内部共変量シフトと呼ぶ。
内部共変量シフトを防ぐために出力値の分布の偏りを抑制する
バッチ正規化が 2015 年に提案されている



プーリング層　畳み込み層の出力を圧縮するプーリング
周りの値をp乗しその標準偏差をとるLpプーリング
周りの最大値で圧縮する最大プーリング
平均値で圧縮する平均プーリング





　
また,近年は,ハイパーパラメータを含め最適化問題とする　ベイズ最適化　が効率的なチューニング方法として注目をあびている.
近年では、ベイズ最適化もハイパーパラメータの最適化の方法として注目されている。これは過去の試行結果から次に行う範囲を確率分布を用いて計算する手法である。


ファインチューニング　既存のモデルの一部を利用して新たなモデルを解くために再学習する手法である。


3 層の自己符号化器は　主成分分析（PCA）　と同様の結果を返す





自己符号化器を多層化すると,ディープニューラルネット同様に勾配消失問題が生じるため,複雑な内部表現を得ることは困難であった.
この問題に対して 2006 年頃に　Hinton　らは,単層の自己符号化器に分割し入力層から繰り返し学習させる　誤差逆伝播法　を積層自己符号化器に適用することで,　汎用的な自己符号化器の利用を可能とした.

また,自己符号化器の代表的な応用例として
・ノイズ除去
・ニューラルネットの事前学習
・異常検知
がある.



再帰型ニューラル・ネットワーク (RNN) 　従来型のフィードフォワード・ネットワークとは対照的に) 層内での重み付き結合を考慮に入れる、ニューラル・ネットワーク
RNN ではネットワークにループ構造が含まれる
音声認識では RNN の一種であるエルマン・ネットワークが利用されてきた
RNN の学習には勾配消失問題を避けることのできる通時的誤差逆伝播法（back-propagation through time法）が利用される
RNN の一種である LSTM は機械翻訳や画像からのキャプション生成などに応用できる



エルマン・ネットワーク　単純再帰型ネットワーク

ジョーダン・ネットワーク　隠れ層の履歴を維持するのではなく、出力層を状態層の中に保管するという点で、エルマン・ネットワークとは異なります




勾配降下法にはパラメータの勾配を数値的に求めると計算量が膨大となってしまう問題があり,このような問題を避けるために誤差逆伝播法





勾配の平均と分散をオンラインで推定し利用するAdam





醜いアヒルの子の定理　認知できる全ての客観的な特徴に基づくと全ての対象は同程度に類似している,つまり特徴を選択しなければ表現の類似度に基づく分類は不可能である



ノーフリーランチ定理
全てのタスクに対して常に他よりすぐれている万能的なアルゴリズムは存在しないこと


次元の呪い
データの次元が増加すると,問題の算法が指数関数的に大きくなること


バーニーおじさんのルール
学習に用いるデータ量の目安となる経験則



プーリング層では畳み込み層の出力を圧縮するプーリングを行う
周りの最大値で圧縮する最大プーリング
平均値で圧縮する平均プーリング
周りの値をp乗しその標準偏差をとるLpプーリングなどが存在する。









確定的モデル

畳み込みニューラルネットCNN
積層自己符号化器
再帰型ニューラルネットRNN



確率的モデル
深層ボルツマンマシン（​Boltzmann machine​）





局所コントラスト正規化
画像処理の分野においては,減算正規化と除算正規化の処理を行う前処理







bag-of-words
テキストデータを数値化



TF-IDF
文章に含まれる単語の重要度を特徴量とする




蒸留　はモデルのパラメータを小さくする手法の一つである
大きなニューラルネットワークなどの入出力をより小さなネットワークに学習させる技術
すでに学習されているモデル（教師モデル）を利用して,より小さくシンプルなモデル（生徒モデル）を学習させる手法である.
生徒モデルを単独で学習させる場合よりも　過学習を緩和する　ことができる.



ハイパーパラメータのチューニング方法としては,パラメータの候補値を指定し,それらの組み合わせを調べる　ベイズ最適化


ファインチューニング
既存のモデルの一部を利用して新たなモデルを解くために再学習する手法である。
転移学習の一部利用版？














word2vec





形態素解析




adversarial example
学習済みのディープニューラルネットモデルを欺くように人工的に作られたサンプルのことである.サンプルに対して微小な摂動を加えることで,作為的にモデルの誤認識を引き起こすことができる.






以前に適用した勾配の方向を現在のパラメータ更新にも影響させる モメンタム という手法



勾配を2乗した値を蓄積し,すでに大きく更新されたパラメータほど更新量（学習率）を小さくする
AdaGrad


RMSprop AdaGradにおける一度更新量が飽和した重みはもう更新されないという欠点を,指数移動平均を蓄積することにより解決した




google 社提供の TensorFlow
TensorFlow は keras 
PreferredNetworks  Chainer 

PyTorch　は　Chainer　と同じ Define-by-Run 方式を採用している



パラメータがつくる空間が高次元になり,その空間内の局所最適解や　鞍点　にトラップされることが多くなる.




機械学習において,重み更新に関わる単位として,　イテレーション　と　エポック　がある.
　イテレーション　は,重みが更新された回数であり,　エポック　は訓練データを何回繰り返し学習したかを表す単位である.また一回の　イテレーション　に用いるサンプル数は　バッチサイズ と呼ばれる.








イアン・J・グッドフェロー（Ian J. Goodfellow）
現在はGoogleの人工知能研究チームであるGoogle Brain（英語: Google Brain）のリサーチ・サイエンティスト
ニューラルネットワークを用いた生成モデルの一種である敵対的生成ネットワークを提案したことで知られる








単純パーセプトロンの出力層では　ステップ関数　が用いられる


ニューラルネットワークの中間層では,はじめ　シグモイド関数　などの正規化の機能を持つ関数が好まれた.
しかし現在では,誤差逆伝播で勾配が消失しやすいという問題から,中間層では勾配消失問題の影響を抑えられ,
かつ簡単な ReLU などが用いられている



出力層では出力の総和が 1 になるため確率的な解釈が可能になる　ソフトマックス関数　がよく用いられる.


多項式基底関数　線形基底関数モデルのこと？基底関数を線形結合した線形回帰モデルが線形基底関数











グレースケール化
カラー画像を白黒画像に変換して計算量を削減する
画像データに対しては,前処理を施すことが多い


平滑化
細かいノイズの影響を除去する



ヒストグラム平均
画素ごとの明るさをスケーリングする




seq2seq 翻訳の元の単語たちはベクトル変換されたあと、さらに非線形関数で変換され、DNNに入れられる。ベクトル変換のためにword2vecが使われることもある。




畳み込みニューラルネットワークの　畳み込み層　のパラメータ数は　全結合層　と比較して極めて少ない
これは　重み共有　によって　有用な特徴量を画像の位置によって大きく変化させない　ため,パラメータ数が減り,計算量が少なくなるためである.


重み共有(weight sharing, weight typing)



一般社団法人人工知能学会は,9 つの指針

1 （人類への貢献）
2 （法規制の遵守）
3 （他者のプライバシーの尊重）
4 （公正性）
5 （安全性）
6 （誠実な振る舞い）
7 （社会に対する責任）
8 （社会との対話と自己研鑽）
9 （人工知能への倫理遵守の要請）





スティーブン・ホーキング（宇宙物理学博士）
人工知能の進化は人類の終焉を意味する
完全な人口知能を開発できたら人類の終焉を意味するかもしれない
2014 年のテレビインタビュー


イーロン・マスク（テスラ社長 スペースX CEO）
人工知能にはかなり慎重に取り組む必要がある
結果的に悪魔を呼び出していることになるからだ
ペンタグラムと盛衰を手にした少年が悪魔に立ち向かう話をみなさんご存じだろう



ビル・ゲイツ　マイクロソフト社長
私も人工知能に懸念を抱く側にいる一人だ
脅威論に賛成



ニック・ボストロム（哲学者）
、オックスフォード大学教授





1980 年代に登場し,特定領域に限り実用的成果をあげた AI の研究に関して,適切な選択肢を 1 つ選べ.
エキスパートシステム
エキスパートシステムは、特定領域の知識について質問に答えたり問題を解いたりするプログラムで、専門家の知識から抽出した論理的ルールを使用する。初期の例として、エドワード・ファイゲンバウムらが開発した分光計の計測結果から化合物を特定するDendral(1965)[111]、伝染性血液疾患を診断するMycin(1972)がある。それらがこのアプローチの有効性を示した[112]。 





Facebook 社が招いたディープラーニングの研究者として,正しい人物を選択肢から 1 つ選べ.
Yann LeCun








テリー・ウィノグラード （Terry Allen Winograd
スタンフォード大学の計算機科学者。 SHRDLU とよばれるシステムを用いた自然言語についての研究により人工知能の分野で知られるようになったが、後にこの研究を通じて人工知能の実現には批判的な立場を取るようになった。 














データバイアス問題　学習元のデータが不均衡なデータであった場合、予期していない、データとして存在していない、ような場合の判断が公平な判断ではなくなってしまうことが考えられる。
台風状況を把握しようとデータを集めた時、SNSから集めたので携帯所有者の多い地域しか把握できなかった。
回答してくれた人は、回答してくれなかった人と根本的に考え方が違ったり、思想的な偏りがあった(協力的な人は優しい性格、警戒心の弱い人だけが回答してくれる、など。)。



データ拡張　data augmentation データを変形したりして水増しする方法
データ量が少ないと過学習が生じるため、過学習対策になる。

DNNの転移学習 transfer learning 学習済みDNNを新しい認識対象に対して使う。
微調整 fine tuning　転移学習するときに少しだけ層を追加したり、パラメータを学習させなおしたりする
転移学習は大量のデータから学習し終わったネットワークを、別の画像に適用してもうまく働く場合があり、メリットとしてはパラメータが学習済なので少量のデータでネットワークが構築できうｒ。



VGG16 オックスフォード大学　2014年のILSVRCで二位　が転移学習の対象としてよく使われる


CGM consumer generated media ブログやSNSなど、各個人が情報発信するメディア　2000年代から登場

2001年　共同執筆百科事典　wikipedia 
LOD linked open data　2007年　データのwebである。 1999年にティム・バーナーズ・リーが提唱したHTMLでなくRDFというデータ記述形式でコンピュータが理解できるようなデータにしようという考えから進展した概念。

セマンティックweb  HTMLでなくRDFというデータ記述形式でコンピュータが理解できるようなデータにしよう  ティム・バーナーズ・リーがネット上はそうあるべきと考えたもの.





wikipediaをLOD形式にしたのがDBpedia












自動運転232 270

社会的意思決定システム　ペア敗者　一体一では敗者になっても複数登場する場合には勝つことがある。選挙区とか、どちらの等も良いとは思えないが片方の党は絶対に嫌という人たちをしらべることができる。



共有型経済 uber airbnb

フィンテック　ICTを駆使した革新的（innovative）、あるいは破壊的（disruptive）な金融商品・サービスの潮流





IPA 2020年までに先端IT人材が4.8万人不足する

先端IT人材　ビックデータ　IoT　人工知能　を担当できる人材

2017年6月　未来投資戦略2017 society 5.0にむけて教育・人材力の抜本強化
2018年6月　未来投資戦略2018 AI時代に対応した人材育成と最適活用

2002年　経済産業省　ITスキル標準 ITSS

2017年4月　第四次産業革命にむけて　ITSS+ を策定　データサイエンス及びセキュリティ領域を対象としている。
2018年4月にはIoTソリューションとアジャイル領域を追加
データサイエンス領域はIPAと一般社団法人データサイエンティスト協会との協業での策定

AIの知識リテラシーを持った人材、実装できる人材としてG検定・E資格がある。

2020年　小学校でのプログラミング教育必修化


日立700-3000 2021
IBM5000人
東レ　昭和電工　富士フィルム　三井化学　1500人




学びなおし

人生100年時代　基礎能力OSを身に着けつ、専門性は各業界でアプリ(能力)としてアップデートするもの　




リカレント教育　web 研修　ライブ配信

全結合層　線形代数としての行列計算と行列積の計算がひつようであり、計算APIはBLASやGEMMが使われる。



SGD sthochastic gradiendt descent 確率的勾配降下法　DNNのパラメータの最適化に使われる。


バッチ学習はデータ全体に勾配降下法(最小二乗法で勾配を求めて最小になるように学習率を設定して更新する最適化手法。)するので計算量がやばくて収まらない。

データ一つだけに対してやるとSGDっぽくて、複数個のデータに対してやるのがミニバッチ学習。

time-to-epoch 1エポックのために必要な時間

epoch  訓練データを何度学習に利用したか

イテレーション  重みを何度更新したか

逐次学習  イテレーション回数=データサイズn 一個づつ使って学習する。　ミニバッチサイズ1

バッチ学習  訓練データすべての誤差で1回更新。（エポック回数＝イテレーション回数）


epoch-to-convergence 満足な条件に収束するために必要なエポック　アーリーストッピングを使うと必要以上に収束させないようにしてくれる


GPU graphic processing unit グラフィック用のプロセッサー

GPGPU general-purpose computing on GPU グラフィック以外にも汎用の計算機能として利用するためのプログラム環境のこと。FPUと似ている。
NVIDIAのCUDA compute unified device architecture とか　AMDのAMD stream, クロノス・グループのOpenCL などが同じようなプログラム環境として有名。

NVIDIAはteslaというDL用のGPUを開発してる



AI企業のディープラーニングのライブラリやフレームワークがOSSオープンソースソフトウェアとして公開されている。

caffe
berkeley社
画像処理向け

tensorflow
google社
DNNの構造を文字で書き下せる

chainer
PFN(preferred networks)
使いやすい　動的ネットワーク構築(define by run)方式
define by runは計算グラフ（ニューラルネットの構造）の構築をデータを流しながら行います


CNTK(computational network toolkit)
microsoft C++で使える
RNNに強い

MXNet
apatch財団(ワシントン　カーネギーメロン)
AWSと相性がいい
前駆としてamazonのDSSTNEがあった。



































































